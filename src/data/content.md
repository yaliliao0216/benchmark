# LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code

## Introduction

我只是一个测试的数据展示，LiveCodeBench is a comprehensive benchmark for evaluating large language models on code-related tasks. Our benchmark is designed to be:

- **Contamination-free**: All problems are sourced from recent programming contests
- **Holistic**: Covers multiple aspects of coding ability
- **Dynamic**: Regularly updated with new problems
- **Challenging**: Tests models across different difficulty levels

## Methodology

We evaluate models across four difficulty levels:

### Level 1: Basic Programming

- Simple algorithmic problems
- Basic data structure operations
- String and array manipulations

### Level 2: Intermediate Programming

- Advanced data structures
- Graph algorithms
- Dynamic programming basics

### Level 3: Advanced Programming

- Complex algorithmic challenges
- Optimization problems
- Advanced dynamic programming

### Level 4: Expert Programming

- Competitive programming level
- Mathematical algorithms
- Complex system design

## Evaluation Metrics

- **Overall Score**: Weighted average across all levels
- **Number of Events**: Total test cases attempted
- **Level-specific Scores**: Performance at each difficulty level

## Agent Frameworks

We evaluate models using various agent frameworks:

- **AutoGPT**: Autonomous task execution
- **LangChain**: Modular AI application framework
- **CrewAI**: Multi-agent collaboration platform

Each framework brings different strengths and approaches to problem-solving, allowing us to assess model performance across diverse execution environments.
